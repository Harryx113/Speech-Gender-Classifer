{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Speech Project.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO7cr5eUD6Cq",
        "colab_type": "code",
        "outputId": "66dbb402-7727-4dae-a74e-e0115731adb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import torchaudio\n",
        "import sys,os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy.random import randint\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import librosa.display\n",
        "import re\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# models\n",
        "from sklearn import linear_model, naive_bayes, neighbors\n",
        "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import svm\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import np_utils\n",
        "from sklearn import metrics \n",
        "from sklearn.linear_model import Perceptron\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "\n",
        "# stat libraries\n",
        "from scipy import stats\n",
        "\n",
        "# Libraries for the evaluation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import model_selection\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.nearest_centroid module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EFIze0lggyi",
        "colab_type": "code",
        "outputId": "5402ff8e-673c-4669-806d-cba73e2173f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H59wtYboT1ib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/Speech Project/recordings.zip\", 'r')\n",
        "zip_ref.extractall(\"/content\")\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxFKEyQMEtg_",
        "colab_type": "code",
        "outputId": "ec523f66-77d1-4dee-ad13-d049c9d2dc38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "speakers_clean = pd.read_csv('/content/drive/My Drive/Speech Project/speakers_clean.csv')\n",
        "speakers_clean"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>age_onset</th>\n",
              "      <th>birthplace</th>\n",
              "      <th>filename</th>\n",
              "      <th>native_language</th>\n",
              "      <th>sex</th>\n",
              "      <th>speakerid</th>\n",
              "      <th>country</th>\n",
              "      <th>file_missing?</th>\n",
              "      <th>continent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>27.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>virginia, south africa</td>\n",
              "      <td>afrikaans1</td>\n",
              "      <td>afrikaans</td>\n",
              "      <td>female</td>\n",
              "      <td>1</td>\n",
              "      <td>south africa</td>\n",
              "      <td>False</td>\n",
              "      <td>Africa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>40.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>pretoria, south africa</td>\n",
              "      <td>afrikaans2</td>\n",
              "      <td>afrikaans</td>\n",
              "      <td>male</td>\n",
              "      <td>2</td>\n",
              "      <td>south africa</td>\n",
              "      <td>False</td>\n",
              "      <td>Africa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>43.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>pretoria, transvaal, south africa</td>\n",
              "      <td>afrikaans3</td>\n",
              "      <td>afrikaans</td>\n",
              "      <td>male</td>\n",
              "      <td>418</td>\n",
              "      <td>south africa</td>\n",
              "      <td>False</td>\n",
              "      <td>Africa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>26.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>pretoria, south africa</td>\n",
              "      <td>afrikaans4</td>\n",
              "      <td>afrikaans</td>\n",
              "      <td>male</td>\n",
              "      <td>1159</td>\n",
              "      <td>south africa</td>\n",
              "      <td>False</td>\n",
              "      <td>Africa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>19.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>cape town, south africa</td>\n",
              "      <td>afrikaans5</td>\n",
              "      <td>afrikaans</td>\n",
              "      <td>male</td>\n",
              "      <td>1432</td>\n",
              "      <td>south africa</td>\n",
              "      <td>False</td>\n",
              "      <td>Africa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2135</th>\n",
              "      <td>46.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>lagos, nigeria</td>\n",
              "      <td>yoruba3</td>\n",
              "      <td>yoruba</td>\n",
              "      <td>female</td>\n",
              "      <td>766</td>\n",
              "      <td>nigeria</td>\n",
              "      <td>False</td>\n",
              "      <td>Africa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2136</th>\n",
              "      <td>46.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>lagos, nigeria</td>\n",
              "      <td>yoruba4</td>\n",
              "      <td>yoruba</td>\n",
              "      <td>male</td>\n",
              "      <td>851</td>\n",
              "      <td>nigeria</td>\n",
              "      <td>False</td>\n",
              "      <td>Africa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2137</th>\n",
              "      <td>47.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>ibadan, nigeria</td>\n",
              "      <td>yoruba5</td>\n",
              "      <td>yoruba</td>\n",
              "      <td>female</td>\n",
              "      <td>2023</td>\n",
              "      <td>nigeria</td>\n",
              "      <td>False</td>\n",
              "      <td>Africa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2138</th>\n",
              "      <td>31.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>bethel, alaska, usa</td>\n",
              "      <td>yupik1</td>\n",
              "      <td>yupik</td>\n",
              "      <td>female</td>\n",
              "      <td>571</td>\n",
              "      <td>usa</td>\n",
              "      <td>False</td>\n",
              "      <td>North America</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2139</th>\n",
              "      <td>24.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>bulawayo, zimbabwe</td>\n",
              "      <td>zulu1</td>\n",
              "      <td>zulu</td>\n",
              "      <td>female</td>\n",
              "      <td>406</td>\n",
              "      <td>zimbabwe</td>\n",
              "      <td>False</td>\n",
              "      <td>Africa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2140 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       age  age_onset  ... file_missing?      continent\n",
              "0     27.0        9.0  ...         False         Africa\n",
              "1     40.0        5.0  ...         False         Africa\n",
              "2     43.0        4.0  ...         False         Africa\n",
              "3     26.0        8.0  ...         False         Africa\n",
              "4     19.0        6.0  ...         False         Africa\n",
              "...    ...        ...  ...           ...            ...\n",
              "2135  46.0        5.0  ...         False         Africa\n",
              "2136  46.0       12.0  ...         False         Africa\n",
              "2137  47.0        2.0  ...         False         Africa\n",
              "2138  31.0        1.0  ...         False  North America\n",
              "2139  24.0       14.0  ...         False         Africa\n",
              "\n",
              "[2140 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yjXp4yAK-An",
        "colab_type": "text"
      },
      "source": [
        "#  Data Loading & Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-39PDuH8LBov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_rate = 16000\n",
        "def extract_features(audio):\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        " \n",
        "    return mfccs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TcVPzpcLQxZ",
        "colab_type": "code",
        "outputId": "d0830e12-37a1-479a-f19a-207a4c04663a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "root = \"./recordings/\"\n",
        "part1 = pd.DataFrame(columns=['filename', 'features'])    \n",
        "part1_dict = {}\n",
        "\n",
        "i = 0\n",
        "max_pad_len = 3500\n",
        "maxlen = 0\n",
        "\n",
        "for path, subdirs, files in os.walk(root):\n",
        "    for name in files:\n",
        "        if i == 400:\n",
        "            break\n",
        "        filename = name\n",
        "        sound, sample_rate = librosa.load(os.path.join(path, name))\n",
        "        data = extract_features(sound)\n",
        "        \n",
        "        if maxlen == 0 or maxlen < data.shape[1]:\n",
        "            maxlen = data.shape[1]\n",
        "            \n",
        "        # zero-pad the mfccs features in order to have all compatible shapes for input of the CNN.\n",
        "        # max_pad_len is the biggest number of audio frames\n",
        "        # obtained by extracting features (mfccs) from all the audio files.\n",
        "        #pad_width = max_pad_len - data.shape[1]\n",
        "        #data = np.pad(data, pad_width=((0,0), (0, pad_width)), mode='constant')\n",
        "        \n",
        "        print(i)\n",
        "        part1_dict[filename] = [data]\n",
        "        i += 1\n",
        "\n",
        "print(maxlen)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "2583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaPh646GVrTE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import numpy\n",
        "from json import JSONEncoder\n",
        "\n",
        "class NumpyArrayEncoder(JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, numpy.ndarray):\n",
        "            return obj.tolist()\n",
        "        return JSONEncoder.default(self, obj)\n",
        "  \n",
        "with open('part1.json', 'w') as fp:\n",
        "    json.dump(part1_dict, fp, cls=NumpyArrayEncoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It8EEtYNkqS6",
        "colab_type": "code",
        "outputId": "8591a376-9d57-48d7-bf9c-730e6892ccfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "import json\n",
        "import numpy\n",
        "from json import JSONEncoder\n",
        "\n",
        "class NumpyArrayEncoder(JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, numpy.ndarray):\n",
        "            return obj.tolist()\n",
        "        return JSONEncoder.default(self, obj)\n",
        "  \n",
        "with open('part2.json', 'w') as fp:\n",
        "    json.dump(part2_dict, fp, cls=NumpyArrayEncoder)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-3a0af8d1c363>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'part2.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart2_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNumpyArrayEncoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'part2_dict' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGQaiDiwoVmn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import numpy\n",
        "from json import JSONEncoder\n",
        "\n",
        "class NumpyArrayEncoder(JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, numpy.ndarray):\n",
        "            return obj.tolist()\n",
        "        return JSONEncoder.default(self, obj)\n",
        "        \n",
        "with open('part3.json', 'w') as fp:\n",
        "    json.dump(part3_dict, fp, cls=NumpyArrayEncoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xwbEYpayI6Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('part4.json', 'w') as fp:\n",
        "    json.dump(part4_dict, fp, cls=NumpyArrayEncoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-oBdQMz-sQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('part5.json', 'w') as fp:\n",
        "    json.dump(part5_dict, fp, cls=NumpyArrayEncoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Owdzivw8muSV",
        "colab_type": "code",
        "outputId": "d64a6cc2-f677-40f5-a53c-580465f6745f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  __MACOSX  part1.json  part2.json  recordings  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuCnGWBGJVVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "with open('part1.json') as f:\n",
        "  part1_test = json.load(f)\n",
        "\n",
        "part1_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suwQAzBdKsS1",
        "colab_type": "code",
        "outputId": "7fe64a74-00c8-4676-e26d-2638df381f2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "print(part1_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3zpm9VmCdNj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "part1_df = pd.DataFrame.from_dict(part1_dict, orient='index', columns=['features'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ-1wtz4D3cM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "part1_df.to_csv('part1_df.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzCcS0f5D8JK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "part1_df.loc[:, 'features'] = part1_df['features'].apply(list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOi8xPBUFMAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "part1_df_test = pd.read_csv('part1_df.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAwgMQjn3SST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_part1 = 3666\n",
        "max_part2 = 3334\n",
        "max_part3 = 2800\n",
        "max_part4 = 3348\n",
        "part1.to_csv('feature_part1.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH_dgLGXuJ4o",
        "colab_type": "code",
        "outputId": "572e85ef-a66d-4eb6-f357-9d9480066fb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "root = \"./recordings/\"\n",
        "i = 400\n",
        "maxlen = 0\n",
        "part2_dict = {}\n",
        "\n",
        "for path, subdirs, files in os.walk(root):\n",
        "    for name in files[400:800]:\n",
        "        filename = name\n",
        "        sound, sample_rate = librosa.load(os.path.join(path, name))\n",
        "        data = extract_features(sound)\n",
        "        if maxlen == 0 or maxlen < data.shape[1]:\n",
        "            maxlen = data.shape[1]\n",
        "        print(i)\n",
        "        part2_dict[filename] = [data]\n",
        "        i += 1\n",
        "        \n",
        "print(maxlen)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n",
            "550\n",
            "551\n",
            "552\n",
            "553\n",
            "554\n",
            "555\n",
            "556\n",
            "557\n",
            "558\n",
            "559\n",
            "560\n",
            "561\n",
            "562\n",
            "563\n",
            "564\n",
            "565\n",
            "566\n",
            "567\n",
            "568\n",
            "569\n",
            "570\n",
            "571\n",
            "572\n",
            "573\n",
            "574\n",
            "575\n",
            "576\n",
            "577\n",
            "578\n",
            "579\n",
            "580\n",
            "581\n",
            "582\n",
            "583\n",
            "584\n",
            "585\n",
            "586\n",
            "587\n",
            "588\n",
            "589\n",
            "590\n",
            "591\n",
            "592\n",
            "593\n",
            "594\n",
            "595\n",
            "596\n",
            "597\n",
            "598\n",
            "599\n",
            "600\n",
            "601\n",
            "602\n",
            "603\n",
            "604\n",
            "605\n",
            "606\n",
            "607\n",
            "608\n",
            "609\n",
            "610\n",
            "611\n",
            "612\n",
            "613\n",
            "614\n",
            "615\n",
            "616\n",
            "617\n",
            "618\n",
            "619\n",
            "620\n",
            "621\n",
            "622\n",
            "623\n",
            "624\n",
            "625\n",
            "626\n",
            "627\n",
            "628\n",
            "629\n",
            "630\n",
            "631\n",
            "632\n",
            "633\n",
            "634\n",
            "635\n",
            "636\n",
            "637\n",
            "638\n",
            "639\n",
            "640\n",
            "641\n",
            "642\n",
            "643\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "648\n",
            "649\n",
            "650\n",
            "651\n",
            "652\n",
            "653\n",
            "654\n",
            "655\n",
            "656\n",
            "657\n",
            "658\n",
            "659\n",
            "660\n",
            "661\n",
            "662\n",
            "663\n",
            "664\n",
            "665\n",
            "666\n",
            "667\n",
            "668\n",
            "669\n",
            "670\n",
            "671\n",
            "672\n",
            "673\n",
            "674\n",
            "675\n",
            "676\n",
            "677\n",
            "678\n",
            "679\n",
            "680\n",
            "681\n",
            "682\n",
            "683\n",
            "684\n",
            "685\n",
            "686\n",
            "687\n",
            "688\n",
            "689\n",
            "690\n",
            "691\n",
            "692\n",
            "693\n",
            "694\n",
            "695\n",
            "696\n",
            "697\n",
            "698\n",
            "699\n",
            "700\n",
            "701\n",
            "702\n",
            "703\n",
            "704\n",
            "705\n",
            "706\n",
            "707\n",
            "708\n",
            "709\n",
            "710\n",
            "711\n",
            "712\n",
            "713\n",
            "714\n",
            "715\n",
            "716\n",
            "717\n",
            "718\n",
            "719\n",
            "720\n",
            "721\n",
            "722\n",
            "723\n",
            "724\n",
            "725\n",
            "726\n",
            "727\n",
            "728\n",
            "729\n",
            "730\n",
            "731\n",
            "732\n",
            "733\n",
            "734\n",
            "735\n",
            "736\n",
            "737\n",
            "738\n",
            "739\n",
            "740\n",
            "741\n",
            "742\n",
            "743\n",
            "744\n",
            "745\n",
            "746\n",
            "747\n",
            "748\n",
            "749\n",
            "750\n",
            "751\n",
            "752\n",
            "753\n",
            "754\n",
            "755\n",
            "756\n",
            "757\n",
            "758\n",
            "759\n",
            "760\n",
            "761\n",
            "762\n",
            "763\n",
            "764\n",
            "765\n",
            "766\n",
            "767\n",
            "768\n",
            "769\n",
            "770\n",
            "771\n",
            "772\n",
            "773\n",
            "774\n",
            "775\n",
            "776\n",
            "777\n",
            "778\n",
            "779\n",
            "780\n",
            "781\n",
            "782\n",
            "783\n",
            "784\n",
            "785\n",
            "786\n",
            "787\n",
            "788\n",
            "789\n",
            "790\n",
            "791\n",
            "792\n",
            "793\n",
            "794\n",
            "795\n",
            "796\n",
            "797\n",
            "798\n",
            "799\n",
            "3334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVhNMwoYJiun",
        "colab_type": "text"
      },
      "source": [
        "##-----------------------Start Here---------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lrutx0htz7t_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read json to csv\n",
        "part1 = pd.read_json('/content/drive/My Drive/Speech Project/part1.json', orient='index')\n",
        "part2 = pd.read_json('/content/drive/My Drive/Speech Project/part2.json', orient='index')\n",
        "part3 = pd.read_json('/content/drive/My Drive/Speech Project/part3.json', orient='index')\n",
        "part4 = pd.read_json('/content/drive/My Drive/Speech Project/part4.json', orient='index')\n",
        "part5 = pd.read_json('/content/drive/My Drive/Speech Project/part5.json', orient='index')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjU-Yfnz1dOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# concat all\n",
        "features = pd.concat([part1, part2, part3, part4, part5]).reset_index()\n",
        "# change column names\n",
        "features_clean = features.rename(columns={\"index\": \"filename\", 0: \"features\"})\n",
        "# edit file names\n",
        "features_clean['filename'] = features_clean['filename'].apply(lambda x: x[:-4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so9qDNEQ5Q9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read speaker info csv\n",
        "speakers_clean = pd.read_csv('/content/drive/My Drive/Speech Project/speakers_clean.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVd4NcNrAwjV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# combine features with speakers\n",
        "speaker_and_feature = speakers_clean.merge(features_clean, left_on='filename', right_on='filename')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARr2KLzmBgdW",
        "colab_type": "code",
        "outputId": "4e9bb648-d295-4e90-f965-602e85f03a82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(speaker_and_feature.features[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3hlCDmuDrd6",
        "colab_type": "text"
      },
      "source": [
        "# Train / Test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80d4eooCGPsn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#speaker_and_feature = pd.read_csv('speaker_and_feature.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcBDe8EoDs_M",
        "colab_type": "code",
        "outputId": "9340b327-7f92-4c05-abdc-6bba0a459741",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "\n",
        "# use countries of origin as proxies of their races\n",
        "# choose usa(76.5% white), canada, and europe speakers as the training set\n",
        "\n",
        "train_countries = ['usa', 'canada']\n",
        "train_df = pd.concat([speaker_and_feature[speaker_and_feature['country'].isin(train_countries)], \n",
        "                     speaker_and_feature[speaker_and_feature['continent'] == 'Europe']])\n",
        "# obtain test set for white speakers by a 85-15 split \n",
        "train_df, test_df = train_test_split(train_df, test_size=.15)\n",
        "\n",
        "# we use african-country speakers and asian-country speakers only as the test sets\n",
        "test_african_df = speaker_and_feature[speaker_and_feature['continent'] == 'Africa']\n",
        "test_asian_df = speaker_and_feature[speaker_and_feature['continent'] == 'Asia']\n",
        "\n",
        "print('Training size (total)', len(train_df))\n",
        "print('Training size (female)', len(train_df[train_df['sex'] == 'female']))\n",
        "print('Training size (male)', len(train_df[train_df['sex'] == 'male']))\n",
        "\n",
        "print('Test1 size (total)', len(test_df))\n",
        "print('Test1 size (female)', len(test_df[test_df['sex'] == 'female']))\n",
        "print('Test1 size (male)', len(test_df[test_df['sex'] == 'male']))\n",
        "\n",
        "print('\\nTest2 size (african, all)', len(test_african_df))\n",
        "print('Test2 size (african, female)', len(test_african_df[test_african_df['sex'] == 'female']))\n",
        "print('Test2 size (african, male)', len(test_african_df[test_african_df['sex'] == 'male']))\n",
        "\n",
        "print('\\nTest3 size (asian)', len(test_asian_df))\n",
        "print('Test3 size (asian, female)', len(test_asian_df[test_asian_df['sex'] == 'female']))\n",
        "print('Test3 size (asian, male)', len(test_asian_df[test_asian_df['sex'] == 'male']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training size (total) 711\n",
            "Training size (female) 345\n",
            "Training size (male) 366\n",
            "Test1 size (total) 126\n",
            "Test1 size (female) 51\n",
            "Test1 size (male) 75\n",
            "\n",
            "Test2 size (african, all) 189\n",
            "Test2 size (african, female) 72\n",
            "Test2 size (african, male) 117\n",
            "\n",
            "Test3 size (asian) 525\n",
            "Test3 size (asian, female) 271\n",
            "Test3 size (asian, male) 253\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uiq2Ek5RMC0W",
        "colab_type": "code",
        "outputId": "23bb4c8d-ff0a-42db-e836-08dfc310e22a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>age_onset</th>\n",
              "      <th>birthplace</th>\n",
              "      <th>filename</th>\n",
              "      <th>native_language</th>\n",
              "      <th>sex</th>\n",
              "      <th>speakerid</th>\n",
              "      <th>country</th>\n",
              "      <th>file_missing?</th>\n",
              "      <th>continent</th>\n",
              "      <th>features</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>744</th>\n",
              "      <td>18.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>boston, massachusetts, usa</td>\n",
              "      <td>english76</td>\n",
              "      <td>english</td>\n",
              "      <td>female</td>\n",
              "      <td>139</td>\n",
              "      <td>usa</td>\n",
              "      <td>False</td>\n",
              "      <td>North America</td>\n",
              "      <td>[[-545.5300332271895, -544.5147273480371, -545...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399</th>\n",
              "      <td>26.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>myrtle beach, south carolina, usa</td>\n",
              "      <td>english214</td>\n",
              "      <td>english</td>\n",
              "      <td>male</td>\n",
              "      <td>737</td>\n",
              "      <td>usa</td>\n",
              "      <td>False</td>\n",
              "      <td>North America</td>\n",
              "      <td>[[-397.87663064287074, -396.7454699072496, -39...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>608</th>\n",
              "      <td>27.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>ottawa, ontario, canada</td>\n",
              "      <td>english464</td>\n",
              "      <td>english</td>\n",
              "      <td>male</td>\n",
              "      <td>1618</td>\n",
              "      <td>canada</td>\n",
              "      <td>False</td>\n",
              "      <td>North America</td>\n",
              "      <td>[[-493.55471019878087, -492.26183630345247, -4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>674</th>\n",
              "      <td>22.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>plymouth, massachusetts, usa</td>\n",
              "      <td>english535</td>\n",
              "      <td>english</td>\n",
              "      <td>female</td>\n",
              "      <td>1955</td>\n",
              "      <td>usa</td>\n",
              "      <td>False</td>\n",
              "      <td>North America</td>\n",
              "      <td>[[-476.93032868169723, -472.61264640519545, -4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>970</th>\n",
              "      <td>20.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>cosenza, italy</td>\n",
              "      <td>italian14</td>\n",
              "      <td>italian</td>\n",
              "      <td>male</td>\n",
              "      <td>761</td>\n",
              "      <td>italy</td>\n",
              "      <td>False</td>\n",
              "      <td>Europe</td>\n",
              "      <td>[[-569.3366800202974, -569.3366800202974, -569...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      age  ...                                           features\n",
              "744  18.0  ...  [[-545.5300332271895, -544.5147273480371, -545...\n",
              "399  26.0  ...  [[-397.87663064287074, -396.7454699072496, -39...\n",
              "608  27.0  ...  [[-493.55471019878087, -492.26183630345247, -4...\n",
              "674  22.0  ...  [[-476.93032868169723, -472.61264640519545, -4...\n",
              "970  20.0  ...  [[-569.3366800202974, -569.3366800202974, -569...\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaDyQeuq_amU",
        "colab_type": "code",
        "outputId": "d16c9146-de13-470e-c015-49d0e12403cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# clean data\n",
        "test_asian_df.loc[test_asian_df['sex'] == 'famale', 'sex'] = 'female'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:966: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.obj[item] = s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ByeZM4wNKpj",
        "colab_type": "code",
        "outputId": "9f5293fb-9192-42ff-a746-fd8a3b5cd78e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "X_train = np.array(train_df.features.apply(lambda x: np.mean(np.array(x), axis=1)).to_list())\n",
        "X_test = np.array(test_df.features.apply(lambda x: np.mean(np.array(x), axis=1)).to_list())\n",
        "X_test_african = np.array(test_african_df.features.apply(lambda x: np.mean(np.array(x), axis=1)).to_list())\n",
        "X_test_asian = np.array(test_asian_df.features.apply(lambda x: np.mean(np.array(x), axis=1)).to_list())\n",
        "\n",
        "# 0=female, 1=male\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(np.array(train_df.sex.tolist()))\n",
        "y_test = le.fit_transform(np.array(test_df.sex.tolist()))\n",
        "y_test_african = le.fit_transform(np.array(test_african_df.sex.tolist()))\n",
        "y_test_asian = le.fit_transform(np.array(test_asian_df.sex.tolist()))\n",
        "\n",
        "print('Training feature shape:', X_train.shape)\n",
        "print('Training label shape:', y_train.shape)\n",
        "print('Testing (baseline) feature shape:', X_test.shape)\n",
        "print('Testing (baseline) label shape:', y_test.shape)\n",
        "print('\\nTesting feature (african) shape:', X_test_african.shape)\n",
        "print('Testing label (african) shape:', y_test_african.shape)\n",
        "print('\\nTesting feature (asian) shape:', X_test_asian.shape)\n",
        "print('Testing label (asian) shape:', y_test_asian.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training feature shape: (711, 40)\n",
            "Training label shape: (711,)\n",
            "Testing (baseline) feature shape: (126, 40)\n",
            "Testing (baseline) label shape: (126,)\n",
            "\n",
            "Testing feature (african) shape: (189, 40)\n",
            "Testing label (african) shape: (189,)\n",
            "\n",
            "Testing feature (african) shape: (525, 40)\n",
            "Testing label (african) shape: (525,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6rQeSDYM1DF",
        "colab_type": "text"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAqOnI2RMoJW",
        "colab_type": "code",
        "outputId": "488ba9b6-26ab-41fd-e933-86ca96ccf18e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# hyperparameter tuning\n",
        "for n in range(1,21):\n",
        "  knn = neighbors.KNeighborsClassifier(n, weights='distance')\n",
        "  print('N={}, cross validation accuracy='.format(n), np.mean(cross_val_score(knn, X_train, y_train, scoring='accuracy', cv=10)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "N=1, cross validation accuracy= 0.8368348982785603\n",
            "N=2, cross validation accuracy= 0.8368348982785603\n",
            "N=3, cross validation accuracy= 0.8481220657276995\n",
            "N=4, cross validation accuracy= 0.8565336463223787\n",
            "N=5, cross validation accuracy= 0.8537754303599373\n",
            "N=6, cross validation accuracy= 0.8565727699530516\n",
            "N=7, cross validation accuracy= 0.8523474178403756\n",
            "N=8, cross validation accuracy= 0.8607785602503911\n",
            "N=9, cross validation accuracy= 0.8509389671361502\n",
            "N=10, cross validation accuracy= 0.8678403755868545\n",
            "N=11, cross validation accuracy= 0.842488262910798\n",
            "N=12, cross validation accuracy= 0.8565532081377152\n",
            "N=13, cross validation accuracy= 0.853755868544601\n",
            "N=14, cross validation accuracy= 0.8593114241001565\n",
            "N=15, cross validation accuracy= 0.8494718309859154\n",
            "N=16, cross validation accuracy= 0.8508802816901408\n",
            "N=17, cross validation accuracy= 0.8438771517996869\n",
            "N=18, cross validation accuracy= 0.8509194053208138\n",
            "N=19, cross validation accuracy= 0.8438771517996869\n",
            "N=20, cross validation accuracy= 0.8565532081377152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3UrXlJ95T6s",
        "colab_type": "code",
        "outputId": "685a260b-bdb1-4371-8033-e13f9516ea94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "# choose n=8\n",
        "knn = neighbors.KNeighborsClassifier(8, weights='distance')\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "y_pred = knn.predict(X_test)\n",
        "print(\"Confusion Matrix (Baseline):\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nAccuracy: \", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nPrecision: \", precision_score(y_test, y_pred))\n",
        "print(\"\\nRecall: \", recall_score(y_test, y_pred))\n",
        "\n",
        "y_pred_african = knn.predict(X_test_african)\n",
        "print(\"Confusion Matrix (African):\")\n",
        "print(confusion_matrix(y_test_african, y_pred_african))\n",
        "print(\"\\nAccuracy: \", accuracy_score(y_test_african, y_pred_african))\n",
        "print(\"\\nPrecision: \", precision_score(y_test_african, y_pred_african))\n",
        "print(\"\\nRecall: \", recall_score(y_test_african, y_pred_african))\n",
        "\n",
        "y_pred_asian = knn.predict(X_test_asian)\n",
        "print(\"\\nConfusion Matrix (Asian):\")\n",
        "print(confusion_matrix(y_test_asian, y_pred_asian))\n",
        "print(\"\\nAccuracy: \", accuracy_score(y_test_asian, y_pred_asian))\n",
        "print(\"\\nPrecision: \", precision_score(y_test_asian, y_pred_asian))\n",
        "print(\"\\nRecall: \", recall_score(y_test_asian, y_pred_asian))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix (Baseline):\n",
            "[[45 18]\n",
            " [ 4 59]]\n",
            "\n",
            "Accuracy:  0.8253968253968254\n",
            "\n",
            "Precision:  0.7662337662337663\n",
            "\n",
            "Recall:  0.9365079365079365\n",
            "Confusion Matrix (African):\n",
            "[[49 23]\n",
            " [28 89]]\n",
            "\n",
            "Accuracy:  0.7301587301587301\n",
            "\n",
            "Precision:  0.7946428571428571\n",
            "\n",
            "Recall:  0.7606837606837606\n",
            "\n",
            "Confusion Matrix (Asian):\n",
            "[[196  76]\n",
            " [ 36 217]]\n",
            "\n",
            "Accuracy:  0.7866666666666666\n",
            "\n",
            "Precision:  0.7406143344709898\n",
            "\n",
            "Recall:  0.857707509881423\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRYbqtX0CuNL",
        "colab_type": "text"
      },
      "source": [
        "Notice that the confusion matrix here at (0,0), (1,0), (0,1), (1,1) are TN, FN, FP, TP respectively.  \n",
        "Recall that we expect to have more False Positives (true: female, predicted: male) for Africans and more False Negatives (true: male, predicted: female) for Asians.  \n",
        "In this case, the False Positive Rate(FPR) for the baseline is 18/(18+45) = 0.286. The FPR for Africans is 23/(23+49) = 0.319 and the FPR for Asians is 76/(76+196) = 0.279. The False Negative Rate(FNR) for the baseline is 4/(4+59) = 0.063, while the FNR for Africans is 28/(28+89) = 0.239 and for Asians is 30/(30+223) = 0.119.  \n",
        "###Asians are lower in both (compared to African counterparts), so it's 50% consistent with our hypothesis for KNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yrFTqxJGivA",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag0d_eIEGkMR",
        "colab_type": "code",
        "outputId": "fee869a3-2705-449d-bcd2-2920ef51c947",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "nb = naive_bayes.GaussianNB();\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = nb.predict(X_test)\n",
        "print(\"Confusion Matrix (Baseline):\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nAccuracy: \", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nPrecision: \", precision_score(y_test, y_pred))\n",
        "print(\"\\nRecall: \", recall_score(y_test, y_pred))\n",
        "\n",
        "y_pred_african = nb.predict(X_test_african)\n",
        "print(\"Confusion Matrix (African):\")\n",
        "print(confusion_matrix(y_test_african, y_pred_african))\n",
        "print(\"\\nAccuracy: \", accuracy_score(y_test_african, y_pred_african))\n",
        "print(\"\\nPrecision: \", precision_score(y_test_african, y_pred_african))\n",
        "print(\"\\nRecall: \", recall_score(y_test_african, y_pred_african))\n",
        "\n",
        "y_pred_asian = nb.predict(X_test_asian)\n",
        "print(\"\\nConfusion Matrix (Asian):\")\n",
        "print(confusion_matrix(y_test_asian, y_pred_asian))\n",
        "print(\"\\nAccuracy: \", accuracy_score(y_test_asian, y_pred_asian))\n",
        "print(\"\\nPrecision: \", precision_score(y_test_asian, y_pred_asian))\n",
        "print(\"\\nRecall: \", recall_score(y_test_asian, y_pred_asian))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix (Baseline):\n",
            "[[53 10]\n",
            " [ 4 59]]\n",
            "\n",
            "Accuracy:  0.8888888888888888\n",
            "\n",
            "Precision:  0.855072463768116\n",
            "\n",
            "Recall:  0.9365079365079365\n",
            "Confusion Matrix (African):\n",
            "[[ 60  12]\n",
            " [ 12 105]]\n",
            "\n",
            "Accuracy:  0.873015873015873\n",
            "\n",
            "Precision:  0.8974358974358975\n",
            "\n",
            "Recall:  0.8974358974358975\n",
            "\n",
            "Confusion Matrix (Asian):\n",
            "[[244  28]\n",
            " [ 21 232]]\n",
            "\n",
            "Accuracy:  0.9066666666666666\n",
            "\n",
            "Precision:  0.8923076923076924\n",
            "\n",
            "Recall:  0.9169960474308301\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_R_744LeHlOD",
        "colab_type": "text"
      },
      "source": [
        "In this case, the False Positive Rate(FPR) for Africans is 12/(12+60) = 0.167 and the FPR for Asians is 28/(28+244) = 0.103. The False Negative Rate(FNR) for Africans is 12/(12+105) = 0.102 and for Asians is 21/(21+232) = 0.083. \n",
        "\n",
        "The baselines are FPR = 0.159, and FNR = 0.063\n",
        "###Asians are lower in both, so 50% consistent with our hypothesis for Naive Bayes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa1JXzPuIeq1",
        "colab_type": "text"
      },
      "source": [
        "# Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7G6RtAIIVow",
        "colab_type": "code",
        "outputId": "839ec15a-ae1d-4e9a-a59b-b6ba39769520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# hyperparameter tuning\n",
        "\n",
        "C = [0.1, 1, 10, 1e2, 1e3]\n",
        "Gamma = ['scale', 'auto']\n",
        "Kernel = ['poly', 'rbf']\n",
        "\n",
        "for c in C:\n",
        "  for gamma in Gamma:\n",
        "    for kernel in Kernel:\n",
        "      svc = svm.SVC(C=c, kernel=kernel, gamma=gamma)\n",
        "      print('The cv error for C={}'.format(c), 'gamma={}'.format(gamma), 'kernel={}'.format(kernel), 'is', \n",
        "            1-np.mean(cross_val_score(svc, X_train, y_train, scoring='accuracy')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The cv error for C=0.1 gamma=scale kernel=poly is 0.4683541810302374\n",
            "The cv error for C=0.1 gamma=scale kernel=rbf is 0.4683541810302374\n",
            "The cv error for C=0.1 gamma=auto kernel=poly is 0.08865359992120558\n",
            "The cv error for C=0.1 gamma=auto kernel=rbf is 0.4683541810302374\n",
            "The cv error for C=1 gamma=scale kernel=poly is 0.12800157588889982\n",
            "The cv error for C=1 gamma=scale kernel=rbf is 0.15049738993400974\n",
            "The cv error for C=1 gamma=auto kernel=poly is 0.08865359992120558\n",
            "The cv error for C=1 gamma=auto kernel=rbf is 0.3839850290554516\n",
            "The cv error for C=10 gamma=scale kernel=poly is 0.07454939426770402\n",
            "The cv error for C=10 gamma=scale kernel=rbf is 0.07454939426770402\n",
            "The cv error for C=10 gamma=auto kernel=poly is 0.08865359992120558\n",
            "The cv error for C=10 gamma=auto kernel=rbf is 0.3839850290554516\n",
            "The cv error for C=100.0 gamma=scale kernel=poly is 0.07875504776913222\n",
            "The cv error for C=100.0 gamma=scale kernel=rbf is 0.08156209987195917\n",
            "The cv error for C=100.0 gamma=auto kernel=poly is 0.08865359992120558\n",
            "The cv error for C=100.0 gamma=auto kernel=rbf is 0.3839850290554516\n",
            "The cv error for C=1000.0 gamma=scale kernel=poly is 0.08581699990150704\n",
            "The cv error for C=1000.0 gamma=scale kernel=rbf is 0.09283955481138584\n",
            "The cv error for C=1000.0 gamma=auto kernel=poly is 0.08865359992120558\n",
            "The cv error for C=1000.0 gamma=auto kernel=rbf is 0.3839850290554516\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfvul0RtMN3G",
        "colab_type": "code",
        "outputId": "0d32d20b-fd6b-476c-cb03-a01379091b17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "# C=10, gamma=scale, kernel=poly\n",
        "svc = svm.SVC(C=10, kernel='poly', gamma='scale')\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svc.predict(X_test)\n",
        "print(\"Confusion Matrix (Baseline):\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nAccuracy: \", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nPrecision: \", precision_score(y_test, y_pred))\n",
        "print(\"\\nRecall: \", recall_score(y_test, y_pred))\n",
        "\n",
        "y_pred_african = svc.predict(X_test_african)\n",
        "print(\"Confusion Matrix (African):\")\n",
        "print(confusion_matrix(y_test_african, y_pred_african))\n",
        "print(\"\\nAccuracy: \", accuracy_score(y_test_african, y_pred_african))\n",
        "print(\"\\nPrecision: \", precision_score(y_test_african, y_pred_african))\n",
        "print(\"\\nRecall: \", recall_score(y_test_african, y_pred_african))\n",
        "\n",
        "y_pred_asian = svc.predict(X_test_asian)\n",
        "print(\"\\nConfusion Matrix (Asian):\")\n",
        "print(confusion_matrix(y_test_asian, y_pred_asian))\n",
        "print(\"\\nAccuracy: \", accuracy_score(y_test_asian, y_pred_asian))\n",
        "print(\"\\nPrecision: \", precision_score(y_test_asian, y_pred_asian))\n",
        "print(\"\\nRecall: \", recall_score(y_test_asian, y_pred_asian))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix (Baseline):\n",
            "[[54  9]\n",
            " [ 5 58]]\n",
            "\n",
            "Accuracy:  0.8888888888888888\n",
            "\n",
            "Precision:  0.8656716417910447\n",
            "\n",
            "Recall:  0.9206349206349206\n",
            "Confusion Matrix (African):\n",
            "[[ 62  10]\n",
            " [  7 110]]\n",
            "\n",
            "Accuracy:  0.91005291005291\n",
            "\n",
            "Precision:  0.9166666666666666\n",
            "\n",
            "Recall:  0.9401709401709402\n",
            "\n",
            "Confusion Matrix (Asian):\n",
            "[[255  17]\n",
            " [ 15 238]]\n",
            "\n",
            "Accuracy:  0.939047619047619\n",
            "\n",
            "Precision:  0.9333333333333333\n",
            "\n",
            "Recall:  0.9407114624505929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeUUMxmdMsky",
        "colab_type": "text"
      },
      "source": [
        "In this case, the False Positive Rate(FPR) for Africans is 10/(10+62) = 0.139 and the FPR for Asians is 17/(17+255) = 0.0625. The False Negative Rate(FNR) for Africans is 7/(7+110) = 0.060 and for Asians is 15/(15+238) = 0.059\n",
        "\n",
        "Baselines are FPR = 0.143, FNR = 0.079.\n",
        "###Asians are lower in both, so it's 50% consistent with the hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye9B9vP1hi69",
        "colab_type": "text"
      },
      "source": [
        "# Linear Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFdEp9QChjo6",
        "colab_type": "code",
        "outputId": "0b601c57-542b-4f97-b37d-4dc07efc7530",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# hyperparameter tuning\n",
        "alphas = [0.00001,0.00005,0.0001,0.0005,0.001,0.005,0.01,0.05,0.1,0.5,1,5]\n",
        "for alpha in alphas:\n",
        "    perceptron = Perceptron(penalty='l2', alpha=alpha, shuffle=True) # training data are shuffled at each epoch.\n",
        "    perceptron.fit(X_train, y_train)\n",
        "    print('alpha={}, cross validation accuracy='.format(alpha), np.mean(cross_val_score(perceptron, X_train, y_train, scoring='accuracy', cv=10)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "alpha=1e-05, cross validation accuracy= 0.8199334898278561\n",
            "alpha=5e-05, cross validation accuracy= 0.8707550860719875\n",
            "alpha=0.0001, cross validation accuracy= 0.8171752738654146\n",
            "alpha=0.0005, cross validation accuracy= 0.6951095461658843\n",
            "alpha=0.001, cross validation accuracy= 0.7260758998435055\n",
            "alpha=0.005, cross validation accuracy= 0.5190532081377152\n",
            "alpha=0.01, cross validation accuracy= 0.49088419405320816\n",
            "alpha=0.05, cross validation accuracy= 0.5091940532081377\n",
            "alpha=0.1, cross validation accuracy= 0.5007433489827855\n",
            "alpha=0.5, cross validation accuracy= 0.49792644757433485\n",
            "alpha=1, cross validation accuracy= 0.49370109546165875\n",
            "alpha=5, cross validation accuracy= 0.49370109546165875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9auDQGARXWFA",
        "colab_type": "code",
        "outputId": "1fdd949a-eaad-45e9-c27f-06bb55b47662",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "perceptron = Perceptron(penalty='l2', alpha=5e-5, shuffle=True) # training data are shuffled at each epoch.\n",
        "perceptron.fit(X_train, y_train)\n",
        "\n",
        "y_pred = perceptron.predict(X_test)\n",
        "print(\"Confusion Matrix (Baseline):\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nAccuracy: \", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nPrecision: \", precision_score(y_test, y_pred))\n",
        "print(\"\\nRecall: \", recall_score(y_test, y_pred))\n",
        "\n",
        "y_pred_african = perceptron.predict(X_test_african)\n",
        "print(\"Confusion Matrix (African):\")\n",
        "print(confusion_matrix(y_test_african, y_pred_african))\n",
        "print(\"\\nAccuracy: \", accuracy_score(y_test_african, y_pred_african))\n",
        "print(\"\\nPrecision: \", precision_score(y_test_african, y_pred_african))\n",
        "print(\"\\nRecall: \", recall_score(y_test_african, y_pred_african))\n",
        "\n",
        "y_pred_asian = perceptron.predict(X_test_asian)\n",
        "print(\"\\nConfusion Matrix (Asian):\")\n",
        "print(confusion_matrix(y_test_asian, y_pred_asian))\n",
        "print(\"\\nAccuracy: \", accuracy_score(y_test_asian, y_pred_asian))\n",
        "print(\"\\nPrecision: \", precision_score(y_test_asian, y_pred_asian))\n",
        "print(\"\\nRecall: \", recall_score(y_test_asian, y_pred_asian))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix (Baseline):\n",
            "[[60  3]\n",
            " [19 44]]\n",
            "\n",
            "Accuracy:  0.8253968253968254\n",
            "\n",
            "Precision:  0.9361702127659575\n",
            "\n",
            "Recall:  0.6984126984126984\n",
            "Confusion Matrix (African):\n",
            "[[70  2]\n",
            " [38 79]]\n",
            "\n",
            "Accuracy:  0.7883597883597884\n",
            "\n",
            "Precision:  0.9753086419753086\n",
            "\n",
            "Recall:  0.6752136752136753\n",
            "\n",
            "Confusion Matrix (Asian):\n",
            "[[267   5]\n",
            " [ 78 175]]\n",
            "\n",
            "Accuracy:  0.8419047619047619\n",
            "\n",
            "Precision:  0.9722222222222222\n",
            "\n",
            "Recall:  0.691699604743083\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vI1L914iiSBD",
        "colab_type": "text"
      },
      "source": [
        "Precisions are very high and recalls are very low in this case, the False Positive Rate(FPR) for Africans is 2/72 = 0.028 and the FPR for Asians is 5/272=0.018. The False Negative Rate(FNR) for Africans is 38/(38+79) = 0.325 and for Asians is 78/(78+175) = 0.308.  \n",
        "\n",
        "Baselines are FPR = 0.048, FNR = 0.302.\n",
        "###Asians are lower in both, so it's 50% consistent with the hypothesis.  \n",
        "###But this model is biased towards higher precision and lower recall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCOHa9LQkGYU",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn7yLhv8kI6D",
        "colab_type": "code",
        "outputId": "b181613b-1283-4402-e642-0e93579af3b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# hyperparameter tuning\n",
        "\n",
        "C = [0.1, 1, 10, 1e2, 1e3, 1e4]\n",
        "Solver = ['lbfgs', 'saga']\n",
        "\n",
        "for c in C:\n",
        "  for solver in Solver:\n",
        "    logit = linear_model.LogisticRegression(C=c, solver=solver, max_iter=10000)\n",
        "    print('The cv error for C={}'.format(c), 'solver={}'.format(solver), 'is', \n",
        "          1-np.mean(cross_val_score(logit, X_train, y_train, scoring='accuracy')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The cv error for C=0.1 solver=lbfgs is 0.08440854919728158\n",
            "The cv error for C=0.1 solver=saga is 0.08440854919728158\n",
            "The cv error for C=1 solver=lbfgs is 0.08723529991135626\n",
            "The cv error for C=1 solver=saga is 0.08580715059588295\n",
            "The cv error for C=10 solver=lbfgs is 0.08864375061558172\n",
            "The cv error for C=10 solver=saga is 0.08580715059588295\n",
            "The cv error for C=100.0 solver=lbfgs is 0.08723529991135626\n",
            "The cv error for C=100.0 solver=saga is 0.08580715059588295\n",
            "The cv error for C=1000.0 solver=lbfgs is 0.08723529991135626\n",
            "The cv error for C=1000.0 solver=saga is 0.08580715059588295\n",
            "The cv error for C=10000.0 solver=lbfgs is 0.08723529991135626\n",
            "The cv error for C=10000.0 solver=saga is 0.08580715059588295\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_2hit4vvw0a",
        "colab_type": "code",
        "outputId": "6777a34e-547e-48d3-e41b-00b202bf42c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "# C=0.1, solver=saga\n",
        "\n",
        "logit = linear_model.LogisticRegression(C=0.1, solver='saga', max_iter=10000, verbose=0)\n",
        "logit.fit(X_train, y_train)\n",
        "\n",
        "y_pred = logit.predict(X_test)\n",
        "print(\"Confusion Matrix (Baseline):\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nAccuracy: \", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nPrecision: \", precision_score(y_test, y_pred))\n",
        "print(\"\\nRecall: \", recall_score(y_test, y_pred))\n",
        "\n",
        "y_pred_african = logit.predict(X_test_african)\n",
        "print(\"Confusion Matrix (African):\")\n",
        "print(confusion_matrix(y_test_african, y_pred_african))\n",
        "print(\"\\nAccuracy: \", accuracy_score(y_test_african, y_pred_african))\n",
        "print(\"\\nPrecision: \", precision_score(y_test_african, y_pred_african))\n",
        "print(\"\\nRecall: \", recall_score(y_test_african, y_pred_african))\n",
        "\n",
        "y_pred_asian = logit.predict(X_test_asian)\n",
        "print(\"\\nConfusion Matrix (Asian):\")\n",
        "print(confusion_matrix(y_test_asian, y_pred_asian))\n",
        "print(\"\\nAccuracy: \", accuracy_score(y_test_asian, y_pred_asian))\n",
        "print(\"\\nPrecision: \", precision_score(y_test_asian, y_pred_asian))\n",
        "print(\"\\nRecall: \", recall_score(y_test_asian, y_pred_asian))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix (Baseline):\n",
            "[[57  6]\n",
            " [ 3 60]]\n",
            "\n",
            "Accuracy:  0.9285714285714286\n",
            "\n",
            "Precision:  0.9090909090909091\n",
            "\n",
            "Recall:  0.9523809523809523\n",
            "Confusion Matrix (African):\n",
            "[[ 64   8]\n",
            " [  9 108]]\n",
            "\n",
            "Accuracy:  0.91005291005291\n",
            "\n",
            "Precision:  0.9310344827586207\n",
            "\n",
            "Recall:  0.9230769230769231\n",
            "\n",
            "Confusion Matrix (Asian):\n",
            "[[253  19]\n",
            " [ 19 234]]\n",
            "\n",
            "Accuracy:  0.9276190476190476\n",
            "\n",
            "Precision:  0.924901185770751\n",
            "\n",
            "Recall:  0.924901185770751\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3eCtZMexpJM",
        "colab_type": "text"
      },
      "source": [
        "The False Positive Rate(FPR) for Africans is 8/(8+64) = 0.111 and the FPR for Asians is 19/(19+253) = 0.070. The False Negative Rate(FNR) for Africans is 9/(9+108) = 0.077 and for Asians is 15/(15+238) = 0.075.\n",
        "\n",
        "Baselines are FPR = 0.095, FNR = 0.048\n",
        "###Asians are lower in both, so it's 50% consistent with the hypothesis.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4feJGcWzEnq",
        "colab_type": "text"
      },
      "source": [
        "# ANN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhXRhJ3Gxls8",
        "colab_type": "code",
        "outputId": "a3f2036b-b35a-4340-b5b9-460254ee6bc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# hyperparameter tuning\n",
        "\n",
        "Activation = ['identity', 'logistic', 'tanh', 'relu']\n",
        "Solver = ['lbfgs', 'sgd', 'adam']\n",
        "\n",
        "\n",
        "for activation in Activation:\n",
        "  for solver in Solver:\n",
        "    mlp = MLPClassifier(activation=activation, solver=solver, max_iter=10000)\n",
        "    print('The cv error for activation={}'.format(activation), 'solver={}'.format(solver), 'is', \n",
        "          1-np.mean(cross_val_score(mlp, X_train, y_train, scoring='accuracy')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The cv error for activation=identity solver=lbfgs is 0.5246035654486358\n",
            "The cv error for activation=identity solver=sgd is 0.07315079286910287\n",
            "The cv error for activation=identity solver=adam is 0.0872156013001083\n",
            "The cv error for activation=logistic solver=lbfgs is 0.10407761252831682\n",
            "The cv error for activation=logistic solver=sgd is 0.07174234216487752\n",
            "The cv error for activation=logistic solver=adam is 0.07454939426770424\n",
            "The cv error for activation=tanh solver=lbfgs is 0.09987195902688861\n",
            "The cv error for activation=tanh solver=sgd is 0.06889589283955488\n",
            "The cv error for activation=tanh solver=adam is 0.06047473653107471\n",
            "The cv error for activation=relu solver=lbfgs is 0.4006599034768049\n",
            "The cv error for activation=relu solver=sgd is 0.08439869989165771\n",
            "The cv error for activation=relu solver=adam is 0.08161134640007872\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0KLwZbH4Azj",
        "colab_type": "code",
        "outputId": "8a0db59e-6a06-47a6-92c8-b62d7a3e4edb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "# activation=logistic solver=adam\n",
        "\n",
        "mlp = MLPClassifier(activation='tanh', solver='adam', max_iter=10000)\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "y_pred = mlp.predict(X_test)\n",
        "print(\"Confusion Matrix (Baseline):\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nAccuracy: \", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nPrecision: \", precision_score(y_test, y_pred))\n",
        "print(\"\\nRecall: \", recall_score(y_test, y_pred))\n",
        "\n",
        "y_pred_african = mlp.predict(X_test_african)\n",
        "print(\"Confusion Matrix (African):\")\n",
        "print(confusion_matrix(y_test_african, y_pred_african))\n",
        "print(\"\\nAccuracy: \", accuracy_score(y_test_african, y_pred_african))\n",
        "print(\"\\nPrecision: \", precision_score(y_test_african, y_pred_african))\n",
        "print(\"\\nRecall: \", recall_score(y_test_african, y_pred_african))\n",
        "\n",
        "y_pred_asian = mlp.predict(X_test_asian)\n",
        "print(\"\\nConfusion Matrix (Asian):\")\n",
        "print(confusion_matrix(y_test_asian, y_pred_asian))\n",
        "print(\"\\nAccuracy: \", accuracy_score(y_test_asian, y_pred_asian))\n",
        "print(\"\\nPrecision: \", precision_score(y_test_asian, y_pred_asian))\n",
        "print(\"\\nRecall: \", recall_score(y_test_asian, y_pred_asian))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix (Baseline):\n",
            "[[56  7]\n",
            " [ 4 59]]\n",
            "\n",
            "Accuracy:  0.9126984126984127\n",
            "\n",
            "Precision:  0.8939393939393939\n",
            "\n",
            "Recall:  0.9365079365079365\n",
            "Confusion Matrix (African):\n",
            "[[ 66   6]\n",
            " [ 11 106]]\n",
            "\n",
            "Accuracy:  0.91005291005291\n",
            "\n",
            "Precision:  0.9464285714285714\n",
            "\n",
            "Recall:  0.905982905982906\n",
            "\n",
            "Confusion Matrix (Asian):\n",
            "[[257  15]\n",
            " [ 19 234]]\n",
            "\n",
            "Accuracy:  0.9352380952380952\n",
            "\n",
            "Precision:  0.9397590361445783\n",
            "\n",
            "Recall:  0.924901185770751\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jheWwyS24zOK",
        "colab_type": "text"
      },
      "source": [
        "The False Positive Rate(FPR) for Africans is 6/(6+66) = 0.083 and the FPR for Asians is 15/(15+257) = 0.055. The False Negative Rate(FNR) for Africans is 11/(11+106) = 0.094 and for Asians is 19/(19+234) = 0.075.\n",
        "\n",
        "Baselines are FPR = 0.111, FNR = 0.063.\n",
        "###Asians are lower in both, so it's 50% consistent with the hypothesis.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEV5GWOGoA7S",
        "colab_type": "text"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piofsw1DCPMY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df, validation_df = train_test_split(train_df, test_size=.2)\n",
        "X_train_CNN = np.array(train_df.features)\n",
        "X_validation_CNN = np.array(validation_df.features)\n",
        "y_train = le.fit_transform(np.array(train_df.sex.tolist()))\n",
        "y_validation = le.fit_transform(np.array(validation_df.sex.tolist()))\n",
        "X_test_CNN = np.array(test_df.features)\n",
        "X_test_african_CNN = np.array(test_african_df.features)\n",
        "X_test_asian_CNN = np.array(test_asian_df.features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyNGQkmrXVPB",
        "colab_type": "code",
        "outputId": "d5fcf112-ff7f-4b2d-c2ce-59a46ab8461a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# pad mfccs and make tensors\n",
        "import torch\n",
        "def get_tensor(arr):\n",
        "    size = len(arr)\n",
        "    tensor = np.zeros((size,40,3666))\n",
        "    for i in range(size):\n",
        "        for j in range(40):\n",
        "            diff = 3666 - len(arr[i][j])\n",
        "            tensor[i][j] = np.hstack((arr[i][j], np.zeros(diff)))\n",
        "    tensor = tensor.reshape(tensor.shape[0], 1, 40, 3666)\n",
        "    tensor = torch.tensor(tensor, dtype=torch.float)\n",
        "    print(tensor.shape)\n",
        "    return tensor\n",
        "X_train_tensor = get_tensor(X_train_CNN)\n",
        "X_validation_tensor = get_tensor(X_validation_CNN)\n",
        "X_test_tensor = get_tensor(X_test_CNN)\n",
        "X_test_african_tensor = get_tensor(X_test_african_CNN)\n",
        "X_test_asian_tensor = get_tensor(X_test_asian_CNN)\n",
        "y_train_tensor = torch.tensor(y_train)\n",
        "y_validation_tensor = torch.tensor(y_validation)\n",
        "y_test_tensor = torch.tensor(y_test)\n",
        "y_test_african_tensor = torch.tensor(y_test_african)\n",
        "y_test_asian_tensor = torch.tensor(y_test_asian)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([148, 1, 40, 3666])\n",
            "torch.Size([37, 1, 40, 3666])\n",
            "torch.Size([126, 1, 40, 3666])\n",
            "torch.Size([189, 1, 40, 3666])\n",
            "torch.Size([525, 1, 40, 3666])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7EH1f7N9wLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils import data\n",
        "trainset = data.TensorDataset(X_train_tensor, y_train_tensor)\n",
        "trainloader = data.DataLoader(trainset)\n",
        "validationset = data.TensorDataset(X_validation_tensor, y_validation_tensor)\n",
        "validationloader = data.DataLoader(validationset)\n",
        "testset = data.TensorDataset(X_test_tensor, y_test_tensor)\n",
        "testloader = data.DataLoader(testset)\n",
        "testset_african = data.TensorDataset(X_test_african_tensor, y_test_african_tensor)\n",
        "testloader_african = data.DataLoader(testset_african)\n",
        "testset_asian = data.TensorDataset(X_test_asian_tensor, y_test_asian_tensor)\n",
        "testloader_asian = data.DataLoader(testset_asian)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiEhDIgdoGWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, ch1, ch2):\n",
        "        super(Net, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            # input 40 * 3666\n",
        "            nn.Conv2d(1, ch1, (2,6), stride=(1,3), padding=0), # output 39 * 1221\n",
        "            nn.BatchNorm2d(ch1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(3, stride=3) # 13 * 404\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(ch1, ch2, (3,4), stride=(1,2), padding=(1,0)), # 12 * 201 \n",
        "            nn.BatchNorm2d(ch2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, stride=2)\n",
        "        )\n",
        "        self.fc = nn.Linear(ch2 * 6 * 101, 2)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = x.view(x.size(0),-1)\n",
        "        x = self.fc(x)\n",
        "        return x "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flkhs3wYj_0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(ch1, ch2, epochs):\n",
        "    global accuracies\n",
        "    net = Net(ch1,ch2)\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "    losses = []\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "            inputs, labels = data # data is a list of [inputs, labels]\n",
        "            optimizer.zero_grad() # zero the parameter gradients\n",
        "\n",
        "            outputs = net(inputs) # fwd\n",
        "            l = torch.tensor(labels, dtype=torch.long)\n",
        "            loss = criterion(outputs, l)\n",
        "            loss.backward()       # backwd\n",
        "            optimizer.step()      # optimize\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 200 == 199:    # print every 200 mini-batches\n",
        "                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 200))\n",
        "                losses.append(running_loss / 200)\n",
        "                running_loss = 0.0\n",
        "    print('Finished Training')\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in validationloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(\"Validation accuracy for ch1={} and ch2={}: \".format(ch1, ch2), accuracy)\n",
        "    accuracies.append(accuracy)\n",
        "    return net, losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD4npYp4oCds",
        "colab_type": "code",
        "outputId": "10cb0e76-0934-4b56-a90c-778fa8770f44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        }
      },
      "source": [
        "accuracies = []\n",
        "for ch1 in [4,8,16,32,64]:\n",
        "    for ch2 in [4,8,16,32,64]:\n",
        "        train(ch1, ch2, epochs=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n",
            "Validation accuracy for ch1=4 and ch2=4:  86.48648648648648\n",
            "Finished Training\n",
            "Validation accuracy for ch1=4 and ch2=8:  70.27027027027027\n",
            "Finished Training\n",
            "Validation accuracy for ch1=4 and ch2=16:  62.16216216216216\n",
            "Finished Training\n",
            "Validation accuracy for ch1=4 and ch2=32:  97.29729729729729\n",
            "Finished Training\n",
            "Validation accuracy for ch1=4 and ch2=64:  56.75675675675676\n",
            "Finished Training\n",
            "Validation accuracy for ch1=8 and ch2=4:  51.351351351351354\n",
            "Finished Training\n",
            "Validation accuracy for ch1=8 and ch2=8:  51.351351351351354\n",
            "Finished Training\n",
            "Validation accuracy for ch1=8 and ch2=16:  91.89189189189189\n",
            "Finished Training\n",
            "Validation accuracy for ch1=8 and ch2=32:  94.5945945945946\n",
            "Finished Training\n",
            "Validation accuracy for ch1=8 and ch2=64:  91.89189189189189\n",
            "Finished Training\n",
            "Validation accuracy for ch1=16 and ch2=4:  83.78378378378379\n",
            "Finished Training\n",
            "Validation accuracy for ch1=16 and ch2=8:  91.89189189189189\n",
            "Finished Training\n",
            "Validation accuracy for ch1=16 and ch2=16:  54.054054054054056\n",
            "Finished Training\n",
            "Validation accuracy for ch1=16 and ch2=32:  97.29729729729729\n",
            "Finished Training\n",
            "Validation accuracy for ch1=16 and ch2=64:  94.5945945945946\n",
            "Finished Training\n",
            "Validation accuracy for ch1=32 and ch2=4:  51.351351351351354\n",
            "Finished Training\n",
            "Validation accuracy for ch1=32 and ch2=8:  54.054054054054056\n",
            "Finished Training\n",
            "Validation accuracy for ch1=32 and ch2=16:  89.1891891891892\n",
            "Finished Training\n",
            "Validation accuracy for ch1=32 and ch2=32:  56.75675675675676\n",
            "Finished Training\n",
            "Validation accuracy for ch1=32 and ch2=64:  94.5945945945946\n",
            "Finished Training\n",
            "Validation accuracy for ch1=64 and ch2=4:  51.351351351351354\n",
            "Finished Training\n",
            "Validation accuracy for ch1=64 and ch2=8:  81.08108108108108\n",
            "Finished Training\n",
            "Validation accuracy for ch1=64 and ch2=16:  81.08108108108108\n",
            "Finished Training\n",
            "Validation accuracy for ch1=64 and ch2=32:  81.08108108108108\n",
            "Finished Training\n",
            "Validation accuracy for ch1=64 and ch2=64:  91.89189189189189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RYFy-6yiPdZ",
        "colab_type": "text"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpEqi1bLyVr7",
        "colab_type": "code",
        "outputId": "6ff48c7b-fa44-42f1-a506-f40388fccab6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# store validation accuracies during training \n",
        "accuracies_record = accuracies\n",
        "\n",
        "# train model with best hyperparameters\n",
        "net = Net(16,32)\n",
        "accuracies = []\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "losses = []\n",
        "for epoch in range(5):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "        inputs, labels = data # data is a list of [inputs, labels]\n",
        "        optimizer.zero_grad() # zero the parameter gradients\n",
        "\n",
        "        outputs = net(inputs) # fwd\n",
        "        l = torch.tensor(labels, dtype=torch.long)\n",
        "        loss = criterion(outputs, l)\n",
        "        loss.backward()       # backwd\n",
        "        optimizer.step()      # optimize\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:    # print every 100 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
        "            losses.append(running_loss / 100)\n",
        "            running_loss = 0.0\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1,   100] loss: 7.388\n",
            "[2,   100] loss: 0.166\n",
            "[3,   100] loss: 0.140\n",
            "[4,   100] loss: 0.066\n",
            "[5,   100] loss: 0.055\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVyfawGgyWW7",
        "colab_type": "code",
        "outputId": "a63514d0-b17e-4198-d69a-a3187fe7b1ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "with torch.no_grad():\n",
        "    correct, total = 0, 0\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(\"Test accuracy for baseline: \", accuracy)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "    correct, total = 0, 0\n",
        "    for data in testloader_african:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(\"Test accuracy for African: \", accuracy)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "    correct, total = 0, 0\n",
        "    for data in testloader_asian:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(\"Test accuracy for Asian: \", accuracy)\n",
        "    accuracies.append(accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy for baseline:  93.65079365079364\n",
            "Test accuracy for African:  87.83068783068784\n",
            "Test accuracy for Asian:  92.95238095238095\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSde0qeeKH2B",
        "colab_type": "text"
      },
      "source": [
        "## Again, the error rate is lower for the Asian test set than for the African test set, although both are higher than the error rate for the baseline test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8ZGvfq6Jep4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}